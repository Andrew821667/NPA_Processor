
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import time
from typing import Dict, List, Optional, Any

class ConsolidatedVersionParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –ù–ü–ê –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –°–ü–°
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π
        self.sps_sources = [
            {
                'name': '–°–ü–° –ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–∏',
                'base_url': 'https://pravo.gov.ru/ips/',
                'search_url': 'https://pravo.gov.ru/ips/ext_search.html',
                'parser_method': self._parse_pravo_gov
            },
            {
                'name': '–ì–∞—Ä–∞–Ω—Ç (backup)',
                'base_url': 'http://base.garant.ru/',
                'search_url': 'http://www.garant.ru/search/',
                'parser_method': self._parse_garant
            }
        ]
    
    def find_consolidated_version(self, document_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            document_info: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ (number, name, eoNumber)
            
        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        """
        doc_number = document_info.get('number', '')
        doc_name = document_info.get('name', '')
        
        print(f"üîç –ò—â–µ–º –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é: {doc_number}")
        
        # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
        for source in self.sps_sources:
            try:
                print(f"   üì° –ü—Ä–æ–≤–µ—Ä—è–µ–º {source['name']}...")
                result = source['parser_method'](doc_number, doc_name)
                
                if result:
                    result['source'] = source['name']
                    result['search_term'] = doc_number
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ {source['name']}")
                    return result
                else:
                    print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ {source['name']}")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ {source['name']}: {e}")
                continue
        
        # –ï—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        return self._create_manual_search_instructions(doc_number, doc_name)
    
    def _parse_pravo_gov(self, doc_number: str, doc_name: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –°–ü–° –ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–∏"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫
            search_urls = [
                f"https://pravo.gov.ru/ips/?search_base=LAW&search_text={doc_number}",
                f"https://pravo.gov.ru/ips/?docbody=&nd={doc_number}",
                f"https://pravo.gov.ru/ips/?docbody=&text={doc_number}"
            ]
            
            for search_url in search_urls:
                try:
                    response = self.session.get(search_url, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                        links = soup.find_all('a', href=True)
                        
                        for link in links:
                            href = link.get('href', '')
                            text = link.get_text(strip=True)
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ω–∞—à –¥–æ–∫—É–º–µ–Ω—Ç
                            if (doc_number.lower() in text.lower() or 
                                doc_number.lower() in href.lower()):
                                
                                full_url = urljoin(search_url, href)
                                
                                return {
                                    'url': full_url,
                                    'title': text,
                                    'type': 'consolidated_version',
                                    'format': 'html',
                                    'pdf_available': self._check_pdf_availability(full_url)
                                }
                
                except Exception as e:
                    continue
            
            return None
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ pravo.gov.ru: {e}")
            return None
    
    def _parse_garant(self, doc_number: str, doc_name: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ì–∞—Ä–∞–Ω—Ç (backup –∏—Å—Ç–æ—á–Ω–∏–∫)"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –ì–∞—Ä–∞–Ω—Ç–µ
            # –ü—Ä–∏–º–µ—Ä—ã –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ì–∞—Ä–∞–Ω—Ç–µ
            known_garant_urls = {
                '273-–§–ó': 'http://base.garant.ru/70291362/',
                '44-–§–ó': 'http://base.garant.ru/70353464/',
                '223-–§–ó': 'http://base.garant.ru/12177967/'
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            clean_number = doc_number.replace('-–§–ó', '').replace('‚Ññ', '').strip()
            
            if f"{clean_number}-–§–ó" in known_garant_urls:
                garant_url = known_garant_urls[f"{clean_number}-–§–ó"]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
                response = self.session.get(garant_url, timeout=10)
                if response.status_code == 200:
                    return {
                        'url': garant_url,
                        'title': f'–ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è {doc_number}',
                        'type': 'consolidated_version',
                        'format': 'html',
                        'pdf_available': False,
                        'note': '–î–æ—Å—Ç—É–ø–µ–Ω –≤ –ì–∞—Ä–∞–Ω—Ç–µ (backup –∏—Å—Ç–æ—á–Ω–∏–∫)'
                    }
            
            return None
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ì–∞—Ä–∞–Ω—Ç: {e}")
            return None
    
    def _check_pdf_availability(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ PDF –≤–µ—Ä—Å–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        try:
            response = self.session.get(url, timeout=5)
            if response.status_code == 200:
                content = response.text.lower()
                return ('pdf' in content or 
                       '—Å–∫–∞—á–∞—Ç—å' in content or
                       'download' in content)
        except:
            pass
        return False
    
    def _create_manual_search_instructions(self, doc_number: str, doc_name: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        return {
            'type': 'manual_search_required',
            'instructions': [
                f"1. –û—Ç–∫—Ä–æ–π—Ç–µ –°–ü–° '–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–∏': https://pravo.gov.ru/ips/",
                f"2. –í –ø–æ–ª–µ –ø–æ–∏—Å–∫–∞ –≤–≤–µ–¥–∏—Ç–µ: {doc_number}",
                f"3. –ù–∞–π–¥–∏—Ç–µ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é (—Å–æ –≤—Å–µ–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏)",
                f"4. –°–∫–∞—á–∞–π—Ç–µ PDF –≤–µ—Ä—Å–∏—é –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
            ],
            'search_terms': [doc_number, doc_name],
            'backup_sources': [
                {
                    'name': '–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–ü–ª—é—Å',
                    'url': 'http://www.consultant.ru/',
                    'search_term': doc_number
                },
                {
                    'name': '–ì–∞—Ä–∞–Ω—Ç',
                    'url': 'http://www.garant.ru/',
                    'search_term': doc_number
                }
            ]
        }
    
    def extract_pdf_links(self, page_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ PDF —Ñ–∞–π–ª—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(page_url, timeout=10)
            if response.status_code != 200:
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            pdf_links = []
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                
                if ('.pdf' in href.lower() or 
                    'pdf' in link.get_text(strip=True).lower() or
                    '—Å–∫–∞—á–∞—Ç—å' in link.get_text(strip=True).lower()):
                    
                    full_url = urljoin(page_url, href)
                    pdf_links.append(full_url)
            
            return pdf_links
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è PDF —Å—Å—ã–ª–æ–∫: {e}")
            return []
